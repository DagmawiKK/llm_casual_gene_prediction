{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8O3gFp1sqj3",
        "outputId": "f850e121-100d-42ef-dcc6-c39a6367d237"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  480M    0  480M    0     0  19.5M      0 --:--:--  0:00:24 --:--:-- 20.8M\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "goa_human.gaf.gz already exists.\n",
            "go.obo already exists.\n",
            "desc2024.xml already exists.\n",
            "gwas_catalog.tsv already exists.\n",
            "Ensembl2Reactome_All_Levels.txt already exists.\n",
            "ReactomePathways.txt already exists.\n",
            "9606.protein.links.full.v12.0.txt.gz already exists.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import requests\n",
        "import gzip\n",
        "!curl -L \"https://www.ebi.ac.uk/gwas/api/search/downloads/alternative/\" -o \"/content/drive/MyDrive/biological_data/gwas_catalog.tsv\"\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "DATA_DIR = \"/content/drive/MyDrive/biological_data\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Download helper\n",
        "def download_file(url, filename):\n",
        "    full_path = os.path.join(DATA_DIR, filename)\n",
        "    if not os.path.exists(full_path):\n",
        "        print(f\"Downloading {filename}...\")\n",
        "        r = requests.get(url)\n",
        "        with open(full_path, 'wb') as f:\n",
        "            f.write(r.content)\n",
        "    else:\n",
        "        print(f\"{filename} already exists.\")\n",
        "    return full_path\n",
        "\n",
        "# Download required files\n",
        "files = {\n",
        "    \"goa_human.gaf.gz\": \"http://current.geneontology.org/annotations/goa_human.gaf.gz\",\n",
        "    \"go.obo\": \"http://purl.obolibrary.org/obo/go.obo\",\n",
        "    \"desc2024.xml\": \"https://nlmpubs.nlm.nih.gov/projects/mesh/MESH_FILES/xmlmesh/desc2024.xml\",\n",
        "    \"gwas_catalog.tsv\": \"https://www.ebi.ac.uk/gwas/api/search/downloads/alternative/gwas_catalog_v1.0.2-associations_e114_r2025-05-13.tsv\",\n",
        "    \"Ensembl2Reactome_All_Levels.txt\": \"https://reactome.org/download/current/Ensembl2Reactome_All_Levels.txt\",\n",
        "    \"ReactomePathways.txt\": \"https://reactome.org/download/current/ReactomePathways.txt\",\n",
        "    \"9606.protein.links.full.v12.0.txt.gz\": \"https://stringdb-static.org/download/protein.links.full.v12.0/9606.protein.links.full.v12.0.txt.gz\"\n",
        "}\n",
        "\n",
        "for fname, url in files.items():\n",
        "    download_file(url, fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vZWIPTm2srmR"
      },
      "outputs": [],
      "source": [
        "# Data Parsing & Processing\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "\n",
        "def load_string_edges(path, threshold=700):\n",
        "    edges, nodes = [], {}\n",
        "    with gzip.open(path, 'rt') as f:\n",
        "        next(f)\n",
        "        for line in f:\n",
        "            p1, p2, *_, score = line.strip().split()\n",
        "            score = int(score)\n",
        "            if score >= threshold:\n",
        "                for p in (p1, p2):\n",
        "                    if p not in nodes:\n",
        "                        nodes[p] = len(nodes)\n",
        "                edges.append((nodes[p1], nodes[p2]))\n",
        "    return torch.tensor(edges).t().contiguous(), nodes\n",
        "def load_mesh_vocabulary(mesh_xml_path):\n",
        "    valid_mesh_terms = set()\n",
        "    try:\n",
        "        tree = ET.parse(mesh_xml_path)\n",
        "        root = tree.getroot()\n",
        "        for descriptor in root.findall(\".//DescriptorRecord\"):\n",
        "            mesh_id = descriptor.find(\"DescriptorUI\").text  # e.g., D009369\n",
        "            valid_mesh_terms.add(mesh_id)\n",
        "        print(f\"Loaded {len(valid_mesh_terms)} MeSH terms from {mesh_xml_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing MeSH vocabulary file {mesh_xml_path}: {e}. Proceeding with empty MeSH vocabulary.\")\n",
        "    return valid_mesh_terms\n",
        "\n",
        "def parse_go_annotations(path, node_map):\n",
        "    gene_go = defaultdict(set)\n",
        "    with gzip.open(path, 'rt') as f:\n",
        "        for line in f:\n",
        "            if line.startswith(\"!\"): continue\n",
        "            cols = line.strip().split(\"\\t\")\n",
        "            if len(cols) >= 5:\n",
        "                gene, go_term = cols[1], cols[4]\n",
        "                if gene in node_map:\n",
        "                    gene_go[node_map[gene]].add(go_term)\n",
        "    return gene_go\n",
        "\n",
        "def parse_reactome(path, node_map):\n",
        "    df = pd.read_csv(path, sep='\\t', header=None)\n",
        "    reactome_map = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        gene_id, pathway = row[0], row[1]\n",
        "        if gene_id in node_map:\n",
        "            reactome_map[node_map[gene_id]].add(pathway)\n",
        "    return reactome_map\n",
        "\n",
        "def parse_gwas(path, node_map):\n",
        "    df = pd.read_csv(path, sep='\\t')\n",
        "    snp_count = defaultdict(int)\n",
        "    for _, row in df.iterrows():\n",
        "        genes = str(row.get(\"MAPPED_GENE\", \"\")).split(\",\")\n",
        "        for gene in genes:\n",
        "            gene = gene.strip()\n",
        "            if gene in node_map:\n",
        "                snp_count[node_map[gene]] += 1\n",
        "    return snp_count\n",
        "\n",
        "def parse_mesh_annotations(path, node_map):\n",
        "    gene_mesh_associations = defaultdict(set)\n",
        "    mesh_association_file = os.path.join(DATA_DIR, \"gene_mesh_associations.tsv\")\n",
        "\n",
        "    if not os.path.exists(mesh_association_file):\n",
        "        print(f\"Error: MeSH association file not found at {mesh_association_file}. Please provide a valid gene-MeSH association file.\")\n",
        "        return gene_mesh_associations\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(mesh_association_file, sep='\\t', comment='#')\n",
        "        # Assume columns: GeneID (e.g., 9606.ENSP...), MeSH_ID (e.g., D009369)\n",
        "        for _, row in df.iterrows():\n",
        "            gene_identifier = str(row['GeneID'])\n",
        "            mesh_term = str(row['MeSH_ID'])\n",
        "            # Only include valid MeSH terms from the vocabulary\n",
        "            if gene_identifier in node_map and mesh_term in mesh_vocab:\n",
        "                node_idx = node_map[gene_identifier]\n",
        "                gene_mesh_associations[node_idx].add(mesh_term)\n",
        "        print(f\"Parsed {len(gene_mesh_associations)} gene-MeSH associations from {mesh_association_file}\")\n",
        "    except KeyError as e:\n",
        "        print(f\"Error parsing MeSH annotation file: Column {e} not found. Expected columns: GeneID, MeSH_ID. Proceeding without MeSH features.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing MeSH annotation file {mesh_association_file}: {e}. Proceeding without MeSH features.\")\n",
        "    return gene_mesh_associations\n",
        "\n",
        "\n",
        "def build_feature_matrix(node_map, go_data, reactome_data, gwas_data, mesh_data,\n",
        "                         go_vocab, reactome_vocab, mesh_vocab):\n",
        "    num_nodes = len(node_map)\n",
        "    num_go_features = len(go_vocab)\n",
        "    num_reactome_features = len(reactome_vocab)\n",
        "    num_mesh_features = len(mesh_vocab)\n",
        "\n",
        "    total_features = num_go_features + num_reactome_features + num_mesh_features + 1 # +1 for GWAS\n",
        "\n",
        "    x = torch.zeros((num_nodes, total_features))\n",
        "\n",
        "    for node_idx in range(num_nodes):\n",
        "        current_feature_offset = 0\n",
        "\n",
        "        # GO features\n",
        "        for go_term in go_data.get(node_idx, []):\n",
        "            if go_term in go_vocab:\n",
        "                x[node_idx, go_vocab[go_term]] = 1\n",
        "        current_feature_offset += num_go_features\n",
        "\n",
        "        # Reactome features\n",
        "        for r_term in reactome_data.get(node_idx, []):\n",
        "            if r_term in reactome_vocab:\n",
        "                x[node_idx, current_feature_offset + reactome_vocab[r_term]] = 1\n",
        "        current_feature_offset += num_reactome_features\n",
        "\n",
        "        # MeSH features\n",
        "        for mesh_term in mesh_data.get(node_idx, []):\n",
        "            if mesh_term in mesh_vocab:\n",
        "                x[node_idx, current_feature_offset + mesh_vocab[mesh_term]] = 1\n",
        "\n",
        "        # GWAS feature (always last)\n",
        "        x[node_idx, -1] = gwas_data.get(node_idx, 0) / 10.0\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhifLXd3suT2",
        "outputId": "8771d60f-e5af-486a-f7e4-020e27c40679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_cluster/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n",
            "<ipython-input-3-3792fd546dec>:56: DtypeWarning: Columns (9,11,12,13,23,26,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(path, sep='\\t')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: MeSH association file not found at /content/drive/MyDrive/biological_data/gene_mesh_associations.tsv. Please provide a valid gene-MeSH association file.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 23.7780\n",
            "Epoch 2, Loss: 59.3965\n",
            "Epoch 3, Loss: 40.2925\n",
            "Epoch 4, Loss: 85.2632\n",
            "Epoch 5, Loss: 44.0041\n",
            "Epoch 6, Loss: 30.9751\n",
            "Epoch 7, Loss: 18.5333\n",
            "Epoch 8, Loss: 15.8227\n",
            "Epoch 9, Loss: 14.1565\n",
            "Epoch 10, Loss: 12.1978\n",
            "Epoch 11, Loss: 16.3273\n",
            "Epoch 12, Loss: 8.2704\n",
            "Epoch 13, Loss: 6.0403\n",
            "Epoch 14, Loss: 4.7062\n",
            "Epoch 15, Loss: 4.1399\n",
            "Epoch 16, Loss: 4.1734\n",
            "Epoch 17, Loss: 4.4436\n",
            "Epoch 18, Loss: 4.6465\n",
            "Epoch 19, Loss: 4.7648\n",
            "Epoch 20, Loss: 4.7717\n",
            "Epoch 21, Loss: 4.6187\n",
            "Epoch 22, Loss: 4.2995\n",
            "Epoch 23, Loss: 3.7044\n",
            "Epoch 24, Loss: 2.9916\n",
            "Epoch 25, Loss: 2.2410\n",
            "Epoch 26, Loss: 1.8352\n",
            "Epoch 27, Loss: 2.1429\n",
            "Epoch 28, Loss: 2.5992\n",
            "Epoch 29, Loss: 2.7398\n",
            "Epoch 30, Loss: 2.5016\n",
            "Epoch 31, Loss: 2.0229\n",
            "Epoch 32, Loss: 1.5930\n",
            "Epoch 33, Loss: 1.4154\n",
            "Epoch 34, Loss: 1.3666\n",
            "Epoch 35, Loss: 1.3971\n",
            "Epoch 36, Loss: 1.5212\n",
            "Epoch 37, Loss: 1.6143\n",
            "Epoch 38, Loss: 1.4269\n",
            "Epoch 39, Loss: 0.9639\n",
            "Epoch 40, Loss: 0.7533\n",
            "Epoch 41, Loss: 1.0393\n",
            "Epoch 42, Loss: 1.1502\n",
            "Epoch 43, Loss: 0.9956\n",
            "Epoch 44, Loss: 0.7027\n",
            "Epoch 45, Loss: 0.6334\n",
            "Epoch 46, Loss: 0.7868\n",
            "Epoch 47, Loss: 0.7924\n",
            "Epoch 48, Loss: 0.6371\n",
            "Epoch 49, Loss: 0.5266\n",
            "Epoch 50, Loss: 10.9014\n",
            "Model saved to Drive.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch-scatter torch-sparse torch-geometric torch-cluster -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "\n",
        "# Model & Training\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class FusionGeneGNN(nn.Module):\n",
        "    def __init__(self, num_go_features, num_reactome_features, num_mesh_features, # num_gwas_features is 1\n",
        "                 common_embed_dim, num_attention_heads,\n",
        "                 gnn_hidden_dim, gnn_out_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Store the number of features for each modality for slicing\n",
        "        self.num_go_features = num_go_features\n",
        "        self.num_reactome_features = num_reactome_features\n",
        "        self.num_mesh_features = num_mesh_features\n",
        "        # GWAS features count is assumed to be 1 (the last column)\n",
        "\n",
        "        # --- 1. Projection Layers (Detectives make initial summaries) ---\n",
        "        # Each data type gets its own 'translator' to the common 'detective language' (common_embed_dim)\n",
        "        self.go_proj = nn.Linear(self.num_go_features, common_embed_dim)\n",
        "        self.reactome_proj = nn.Linear(self.num_reactome_features, common_embed_dim)\n",
        "        self.mesh_proj = nn.Linear(self.num_mesh_features, common_embed_dim)\n",
        "        self.gwas_proj = nn.Linear(1, common_embed_dim) # GWAS is a single feature\n",
        "\n",
        "        # --- 2. Multi-Headed Attention Layer (Detectives collaborate and focus) ---\n",
        "        # batch_first=True means input shape is (batch_size/num_genes, sequence_length/num_modalities, feature_dim)\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=common_embed_dim,\n",
        "                                               num_heads=num_attention_heads,\n",
        "                                               batch_first=True)\n",
        "\n",
        "        self.conv1 = GCNConv(common_embed_dim, gnn_hidden_dim)\n",
        "        self.conv2 = GCNConv(gnn_hidden_dim, gnn_out_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x_original, edge_index = data.x, data.edge_index\n",
        "\n",
        "\n",
        "        current_idx = 0\n",
        "        go_feat = x_original[:, current_idx : current_idx + self.num_go_features]\n",
        "        current_idx += self.num_go_features\n",
        "\n",
        "        reactome_feat = x_original[:, current_idx : current_idx + self.num_reactome_features]\n",
        "        current_idx += self.num_reactome_features\n",
        "\n",
        "        mesh_feat = x_original[:, current_idx : current_idx + self.num_mesh_features]\n",
        "\n",
        "        gwas_feat = x_original[:, -1].unsqueeze(1) # GWAS is the last feature, ensure it's 2D (N_genes, 1)\n",
        "\n",
        "        proj_go = F.relu(self.go_proj(go_feat))\n",
        "        proj_reactome = F.relu(self.reactome_proj(reactome_feat))\n",
        "        proj_mesh = F.relu(self.mesh_proj(mesh_feat))\n",
        "        proj_gwas = F.relu(self.gwas_proj(gwas_feat))\n",
        "\n",
        "        modalities_stacked = torch.stack([proj_go, proj_reactome, proj_mesh, proj_gwas], dim=1)\n",
        "\n",
        "        attn_output, attn_weights = self.attention(modalities_stacked, modalities_stacked, modalities_stacked)\n",
        "\n",
        "        fused_x = attn_output.mean(dim=1)\n",
        "\n",
        "        x_gnn = F.relu(self.conv1(fused_x, edge_index))\n",
        "        x_gnn = F.dropout(x_gnn, p=0.3, training=self.training)\n",
        "        final_gene_embeddings = self.conv2(x_gnn, edge_index)\n",
        "\n",
        "        return final_gene_embeddings\n",
        "\n",
        "# Load & build\n",
        "ppi_file = os.path.join(DATA_DIR, \"9606.protein.links.full.v12.0.txt.gz\")\n",
        "edge_index, node_map = load_string_edges(ppi_file)\n",
        "\n",
        "go = parse_go_annotations(os.path.join(DATA_DIR, \"goa_human.gaf.gz\"), node_map)\n",
        "reactome = parse_reactome(os.path.join(DATA_DIR, \"Ensembl2Reactome_All_Levels.txt\"), node_map)\n",
        "gwas = parse_gwas(os.path.join(DATA_DIR, \"gwas_catalog.tsv\"), node_map)\n",
        "mesh = parse_mesh_annotations(os.path.join(DATA_DIR, \"desc2024.xml\"), node_map)\n",
        "\n",
        "go_vocab = {t: i for i, t in enumerate(sorted(set().union(*[go[k] for k in go])))}\n",
        "reactome_vocab = {t: i for i, t in enumerate(sorted(set().union(*[reactome[k] for k in reactome])))}\n",
        "mesh_vocab = {t: i for i, t in enumerate(sorted(set().union(*[mesh[k] for k in mesh])))}\n",
        "\n",
        "x = build_feature_matrix(node_map, go, reactome, gwas, mesh, go_vocab, reactome_vocab, mesh_vocab)\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "COMMON_EMBED_DIM = 128\n",
        "NUM_ATTENTION_HEADS = 4\n",
        "GNN_HIDDEN_DIM = 64\n",
        "GNN_OUT_DIM = 32\n",
        "\n",
        "num_go_features = len(go_vocab)\n",
        "num_reactome_features = len(reactome_vocab)\n",
        "num_mesh_features = len(mesh_vocab)\n",
        "\n",
        "model = FusionGeneGNN(\n",
        "    num_go_features=num_go_features,\n",
        "    num_reactome_features=num_reactome_features,\n",
        "    num_mesh_features=num_mesh_features,\n",
        "    common_embed_dim=COMMON_EMBED_DIM,\n",
        "    num_attention_heads=NUM_ATTENTION_HEADS,\n",
        "    gnn_hidden_dim=GNN_HIDDEN_DIM,\n",
        "    gnn_out_dim=GNN_OUT_DIM\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "for epoch in range(50):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = out.norm(p=2)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Save model and node map\n",
        "model_save_path = os.path.join(DATA_DIR, \"fusion_gene_gnn_bundle.pth\")\n",
        "\n",
        "# Bundle model architecture parameters\n",
        "model_params = {\n",
        "    \"num_go_features\": num_go_features,\n",
        "    \"num_reactome_features\": num_reactome_features,\n",
        "    \"num_mesh_features\": num_mesh_features,\n",
        "    \"common_embed_dim\": COMMON_EMBED_DIM,\n",
        "    \"num_attention_heads\": NUM_ATTENTION_HEADS,\n",
        "    \"gnn_hidden_dim\": GNN_HIDDEN_DIM,\n",
        "    \"gnn_out_dim\": GNN_OUT_DIM\n",
        "}\n",
        "\n",
        "# Create a dictionary to hold everything\n",
        "full_model_bundle = {\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"model_params\": model_params,\n",
        "    \"feature_matrix_x\": x,            # The pre-processed feature matrix\n",
        "    \"edge_index\": edge_index,      # The graph connectivity\n",
        "    \"node_map\": node_map,          # Gene ID to index mapping\n",
        "    \"go_vocab\": go_vocab,          # GO term to index mapping\n",
        "    \"reactome_vocab\": reactome_vocab, # Reactome term to index mapping\n",
        "    \"mesh_vocab\": mesh_vocab       # MeSH term to index mapping\n",
        "}\n",
        "\n",
        "# Save the bundle to a single .pth file\n",
        "torch.save(full_model_bundle, model_save_path)\n",
        "print(f\"Full model bundle saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_G2TMrgsw3V",
        "outputId": "4a9c89e7-7243-4354-a344-ba074c845646"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted causal genes: ['9606.ENSP00000262305', '9606.ENSP00000329419', '9606.ENSP00000158762', '9606.ENSP00000000233', '9606.ENSP00000357048']\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# 🔍 Inference Example API\n",
        "# =========================\n",
        "# Dummy logic: rank top-K genes by embedding similarity to a phenotype\n",
        "def infer_causal_genes(phenotype_terms, top_k=5):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(data)\n",
        "\n",
        "    # Here, phenotype_terms would be a list of GO, Reactome or MeSH terms\n",
        "    pheno_vec = torch.zeros(embeddings.size(1)) # Original way to init pheno_vec\n",
        "    for t in phenotype_terms:\n",
        "        if t in go_vocab:\n",
        "            pheno_vec += x[:, go_vocab[t]].mean(0)\n",
        "        elif t in reactome_vocab:\n",
        "            pheno_vec += x[:, len(go_vocab) + reactome_vocab[t]].mean(0)\n",
        "        elif t in mesh_vocab: # Added MeSH\n",
        "            pheno_vec += x[:, len(go_vocab) + len(reactome_vocab) + mesh_vocab[t]].mean(0)\n",
        "\n",
        "\n",
        "    similarities = torch.matmul(embeddings, pheno_vec)\n",
        "    top_idxs = similarities.topk(min(top_k, embeddings.size(0))).indices # Ensure top_k is not > num embeddings\n",
        "    reverse_map = {v: k for k, v in node_map.items()}\n",
        "    return [reverse_map[i.item()] for i in top_idxs]\n",
        "\n",
        "# Example usage:\n",
        "example_phenotype_terms = [\"GO:0008190\", \"GO:0008150\"] # Original example term\n",
        "\n",
        "candidate_genes = infer_causal_genes(example_phenotype_terms, top_k=5)\n",
        "print(\"Predicted causal genes:\", candidate_genes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8FwKcfhszHj"
      },
      "outputs": [],
      "source": [
        "Predicted causal genes: ['9606.ENSP00000262305', '9606.ENSP00000329419', '9606.ENSP00000158762', '9606.ENSP00000000233', '9606.ENSP00000357048']\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}